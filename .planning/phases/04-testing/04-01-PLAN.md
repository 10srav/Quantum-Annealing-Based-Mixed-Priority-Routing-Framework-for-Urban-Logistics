---
phase: 04-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/test_input_validation.py
autonomous: true

must_haves:
  truths:
    - "Malformed JSON requests return 400 with helpful error messages"
    - "Missing required fields return 400 with field-specific errors"
    - "Out-of-range values (n_nodes > 25, negative values) return 400"
    - "Invalid enum values return 400 with allowed values listed"
  artifacts:
    - path: "backend/tests/test_input_validation.py"
      provides: "Input validation test suite"
      min_lines: 100
  key_links:
    - from: "backend/tests/test_input_validation.py"
      to: "backend/app/main.py"
      via: "FastAPI TestClient"
      pattern: "TestClient.*app"
---

<objective>
Create comprehensive API input validation tests for TEST-01

Purpose: Verify that the API correctly rejects malformed, incomplete, and out-of-range inputs with appropriate 400 responses and helpful error messages.

Output: `backend/tests/test_input_validation.py` with full coverage of validation edge cases
</objective>

<execution_context>
@C:\Users\polli\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\polli\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@backend/app/main.py
@backend/src/data_models.py
@backend/tests/test_api.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create input validation test file with JSON and field tests</name>
  <files>backend/tests/test_input_validation.py</files>
  <action>
Create `backend/tests/test_input_validation.py` with comprehensive validation tests:

**Class: TestMalformedJSON**
- `test_invalid_json_syntax` - Send `{broken json` to /solve, expect 400
- `test_empty_body` - Send empty string to /solve, expect 400
- `test_null_body` - Send `null` to /solve, expect 400
- `test_array_instead_of_object` - Send `[]` to /solve, expect 400

**Class: TestMissingFields**
- `test_solve_missing_graph` - POST /solve with `{}`, expect 400 with "graph" in error
- `test_graph_missing_nodes` - Graph with edges only, expect 400 with "nodes" in error
- `test_graph_missing_edges` - Graph with nodes only, expect 400 with "edges" in error
- `test_node_missing_id` - Node without id field, expect 400
- `test_node_missing_type` - Node without type field, expect 400
- `test_edge_missing_from` - Edge without from field, expect 400
- `test_edge_missing_distance` - Edge without distance field, expect 400

**Class: TestOutOfRangeValues**
- `test_generate_city_n_nodes_too_high` - n_nodes=30 (max is 25), expect 400
- `test_generate_city_n_nodes_too_low` - n_nodes=1 (min is 2), expect 400
- `test_generate_city_n_nodes_zero` - n_nodes=0, expect 400
- `test_generate_city_n_nodes_negative` - n_nodes=-5, expect 400
- `test_generate_city_priority_ratio_too_high` - priority_ratio=1.5, expect 400
- `test_generate_city_priority_ratio_negative` - priority_ratio=-0.1, expect 400
- `test_edge_negative_distance` - Edge with distance=-1, expect 400
- `test_edge_zero_distance` - Edge with distance=0, expect 400 (gt=0 constraint)

**Class: TestInvalidEnumValues**
- `test_invalid_node_type` - type="urgent" instead of priority/normal, expect 400
- `test_invalid_traffic_level` - traffic="extreme" instead of low/medium/high, expect 400
- `test_invalid_solver_type` - solver="neural" instead of quantum/greedy, expect 400
- `test_invalid_traffic_profile` - traffic_profile="extreme", expect 400

Use FastAPI TestClient pattern from existing test_api.py.
Each test should verify:
1. Status code is 400
2. Response is JSON
3. "detail" or "errors" key exists in response
  </action>
  <verify>
Run: `cd backend && python -m pytest tests/test_input_validation.py -v`
All tests should pass.
  </verify>
  <done>
- 20+ input validation test cases exist
- All malformed JSON tests verify 400 response
- All missing field tests verify 400 with field name in error
- All out-of-range tests verify 400 response
- All invalid enum tests verify 400 response
  </done>
</task>

<task type="auto">
  <name>Task 2: Add error message quality assertions</name>
  <files>backend/tests/test_input_validation.py</files>
  <action>
Enhance the test file with additional assertions that verify error message quality:

**Add to TestMissingFields class:**
- For each missing field test, assert the field name appears in the error response
- Example: `assert "nodes" in str(response.json()).lower()` for missing nodes test

**Add to TestOutOfRangeValues class:**
- For n_nodes tests, verify error mentions the valid range (2-25)
- For priority_ratio tests, verify error mentions valid range (0.0-1.0)

**Add helper function at module level:**
```python
def assert_validation_error(response, expected_field: str = None):
    """Assert response is a validation error with optional field check."""
    assert response.status_code == 400
    data = response.json()
    assert "detail" in data or "errors" in data
    if expected_field:
        response_text = str(data).lower()
        assert expected_field.lower() in response_text, \
            f"Expected '{expected_field}' in error: {data}"
```

Use this helper in all tests for consistency.
  </action>
  <verify>
Run: `cd backend && python -m pytest tests/test_input_validation.py -v --tb=short`
All tests pass with improved assertions.
  </verify>
  <done>
- Helper function exists for consistent validation error checking
- All tests use the helper function
- Field names appear in validation error messages
- Range constraints appear in error messages where applicable
  </done>
</task>

</tasks>

<verification>
Run the complete test suite to ensure no regressions:
```bash
cd backend && python -m pytest tests/ -v
```

Verify specific coverage of TEST-01 requirements:
- Malformed JSON: At least 4 test cases
- Missing fields: At least 7 test cases
- Out-of-range values: At least 8 test cases
</verification>

<success_criteria>
1. `backend/tests/test_input_validation.py` exists with 20+ test cases
2. All tests pass when run with pytest
3. Tests cover all TEST-01 success criteria:
   - Malformed JSON returns 400
   - Missing fields return 400 with field names
   - Out-of-range values return 400
4. Error messages are verified to be helpful (contain field names, valid ranges)
</success_criteria>

<output>
After completion, create `.planning/phases/04-testing/04-01-SUMMARY.md`
</output>
