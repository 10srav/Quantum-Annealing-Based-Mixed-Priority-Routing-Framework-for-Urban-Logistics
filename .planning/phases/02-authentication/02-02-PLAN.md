---
phase: 02-authentication
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/src/rate_limit.py
  - backend/src/config.py
  - backend/app/main.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Excessive requests from same API key receive 429 Too Many Requests"
    - "429 response includes Retry-After header"
    - "Rate limit resets after window expires"
    - "Different API keys have independent rate limits"
  artifacts:
    - path: "backend/src/rate_limit.py"
      provides: "Rate limiting configuration and key extraction"
      exports: ["limiter", "get_api_key_from_request"]
    - path: "backend/requirements.txt"
      provides: "slowapi dependency"
      contains: "slowapi"
  key_links:
    - from: "backend/app/main.py"
      to: "backend/src/rate_limit.py"
      via: "limiter middleware and decorators"
      pattern: "@limiter.limit"
---

<objective>
Implement request rate limiting using slowapi to protect API from abuse.

Purpose: Prevent API abuse by throttling excessive requests per API key, returning 429 with Retry-After header when limits exceeded.

Output: Rate limiting active on all protected endpoints, keyed by API key, with configurable limits and proper 429 responses.
</objective>

<execution_context>
@C:\Users\polli\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\polli\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-authentication/02-01-PLAN.md
@backend/app/main.py
@backend/src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install slowapi and create rate limit module</name>
  <files>backend/requirements.txt, backend/src/rate_limit.py, backend/src/config.py</files>
  <action>
1. Add slowapi to `backend/requirements.txt`:
   ```
   slowapi>=0.1.9
   ```

2. Update `backend/src/config.py` Settings class:
   ```python
   # Rate Limiting
   rate_limit_per_minute: int = 60  # Requests per minute per API key
   rate_limit_solver_per_minute: int = 10  # Lower limit for compute-heavy solver endpoints
   ```

3. Create `backend/src/rate_limit.py`:
   ```python
   """
   Rate limiting configuration using slowapi.
   """
   from slowapi import Limiter
   from slowapi.util import get_remote_address
   from starlette.requests import Request

   from src.config import get_settings


   def get_api_key_from_request(request: Request) -> str:
       """
       Extract API key from request for rate limiting.
       Falls back to IP address if no API key (for unauthenticated endpoints).
       """
       api_key = request.headers.get("X-API-Key")
       if api_key:
           return api_key
       return get_remote_address(request)


   settings = get_settings()

   # Create limiter with API key as the rate limit key
   limiter = Limiter(
       key_func=get_api_key_from_request,
       default_limits=[f"{settings.rate_limit_per_minute}/minute"]
   )
   ```

Run `pip install slowapi` or update requirements.
  </action>
  <verify>
- slowapi in requirements.txt
- `backend/src/rate_limit.py` exists with `limiter` and `get_api_key_from_request`
- Settings has `rate_limit_per_minute` and `rate_limit_solver_per_minute`
  </verify>
  <done>
Rate limit module created with API key-based limiting.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate rate limiter into FastAPI application</name>
  <files>backend/app/main.py</files>
  <action>
Modify `backend/app/main.py`:

1. Import rate limiter:
   ```python
   from slowapi import _rate_limit_exceeded_handler
   from slowapi.errors import RateLimitExceeded
   from src.rate_limit import limiter
   ```

2. Add limiter state to app (after app creation):
   ```python
   app.state.limiter = limiter
   ```

3. Add rate limit exceeded handler:
   ```python
   app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
   ```

4. Apply rate limits to endpoints using decorators:

   For standard endpoints (60/min default):
   ```python
   @app.get("/graphs")
   @limiter.limit(f"{settings.rate_limit_per_minute}/minute")
   async def list_graphs(request: Request, _: bool = Depends(verify_api_key)):
   ```

   For compute-heavy solver endpoints (10/min):
   ```python
   @app.post("/solve", response_model=SolverResponse)
   @limiter.limit(f"{settings.rate_limit_solver_per_minute}/minute")
   async def solve_route(request: Request, solver_request: SolverRequest, _: bool = Depends(verify_api_key)):
   ```
   Note: Renamed `request` param to `solver_request` to avoid conflict with `Request` from Starlette.

   Apply limits to:
   - `/solve` - solver limit (10/min)
   - `/compare` - solver limit (10/min)
   - `/generate-city` - standard limit (60/min)
   - `/graphs` - standard limit (60/min)
   - `/graphs/{graph_name}` - standard limit (60/min)

5. Add `Request` parameter to each rate-limited endpoint (required by slowapi):
   ```python
   async def list_graphs(request: Request, ...):
   ```

IMPORTANT: The `Request` parameter must be named exactly `request` for slowapi to work.
  </action>
  <verify>
Check decorators applied:
```bash
grep -n "@limiter.limit" backend/app/main.py
```
Should show 5 endpoints with rate limit decorators.
  </verify>
  <done>
Rate limiting integrated with appropriate limits per endpoint type.
  </done>
</task>

<task type="auto">
  <name>Task 3: Test rate limiting and Retry-After header</name>
  <files>None (testing only)</files>
  <action>
1. Start the server with a test API key configured.

2. Test rate limit exceeded response:
   - Make rapid requests to trigger the limit
   - Verify 429 status code
   - Verify Retry-After header present

3. Test script approach:
   ```bash
   # Rapid requests to solver endpoint (limit 10/min)
   for i in {1..12}; do
     curl -s -o /dev/null -w "%{http_code} " \
       -X POST http://localhost:8000/solve \
       -H "X-API-Key: test-api-key-for-dev" \
       -H "Content-Type: application/json" \
       -d '{"graph":{"nodes":[],"edges":[]},"solver":"greedy"}'
   done
   echo ""
   # Should see 200s then 429s
   ```

4. Verify Retry-After header:
   ```bash
   curl -i -X POST http://localhost:8000/solve \
     -H "X-API-Key: test-api-key-for-dev" \
     -H "Content-Type: application/json" \
     -d '{"graph":{"nodes":[],"edges":[]},"solver":"greedy"}'
   ```
   When rate limited, response should include `Retry-After` header.

5. Verify different API keys have independent limits:
   - Exhaust limit with key A
   - Key B should still work
  </action>
  <verify>
- 429 response returned when limit exceeded
- Retry-After header present in 429 response
- Different API keys rate limited independently
  </verify>
  <done>
Rate limiting working with 429 responses and Retry-After header. AUTH-03 satisfied.
  </done>
</task>

</tasks>

<verification>
1. Rapid requests to /solve exceed limit and return 429
2. 429 response includes Retry-After header with seconds until reset
3. Requests from different API keys are rate limited independently
4. Solver endpoints have stricter limit (10/min) than standard endpoints (60/min)
5. Rate limit resets after window expires
</verification>

<success_criteria>
- AUTH-03 satisfied: Excessive requests throttled with 429 and Retry-After header
- Solver endpoints have lower rate limit (compute protection)
- Standard endpoints have reasonable limit (abuse protection)
- Rate limits keyed by API key, not IP
</success_criteria>

<output>
After completion, create `.planning/phases/02-authentication/02-02-SUMMARY.md`
</output>
